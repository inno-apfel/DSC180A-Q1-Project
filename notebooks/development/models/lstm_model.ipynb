{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282897b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf32cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "KERAS_VERBOSITY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tf.config.list_physical_devices('CPU'))\n",
    "display(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5a195",
   "metadata": {},
   "source": [
    "# DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a21494",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../../src/data/temp/lagged_zbp_totals_with_features.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4d4ab",
   "metadata": {},
   "source": [
    "# DROP NON-NUMERICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_feats = data.columns.drop(['emp_nf', 'qp1_nf', 'ap_nf'])\n",
    "data = data[included_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d373af",
   "metadata": {},
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66801866",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_year = 2018\n",
    "\n",
    "data_train = data[data['year']<=split_year]\n",
    "data_test = data[data['year']>split_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406d0bd",
   "metadata": {},
   "source": [
    "# STANDARDIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = data_train.mean()\n",
    "train_mean['zip'] = 0\n",
    "\n",
    "train_std = data_train.std()\n",
    "train_std['zip'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = (data_train-train_mean)/train_std\n",
    "data_test = (data_test-train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10403ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_est_standardization(val):\n",
    "    return (val*train_std['est'])+train_mean['est']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eb884",
   "metadata": {},
   "source": [
    "# DATA PROCESSING (OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = ColumnTransformer([('onehots', OneHotEncoder(handle_unknown='ignore'), ['zip'])]\n",
    "                             ,remainder = 'passthrough')\n",
    "\n",
    "data_ohe_train = preproc.fit_transform(data_train)\n",
    "\n",
    "feature_names = preproc.get_feature_names_out()\n",
    "feature_names = np.char.replace(feature_names.astype('str'), 'onehots__','')\n",
    "feature_names = np.char.replace(feature_names, 'remainder__','')\n",
    "\n",
    "data_ohe_train = pd.DataFrame(data_ohe_train, columns=feature_names)\n",
    "\n",
    "data_ohe_test = preproc.transform(data_test)\n",
    "data_ohe_test = pd.DataFrame(data_ohe_test, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29545a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acf270",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33029be",
   "metadata": {},
   "source": [
    "# WINDOWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b135ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    \n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                train_df=data_ohe_train, test_df=data_ohe_test,\n",
    "                label_columns=None, batch_size=1):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                          enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "    \n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    \n",
    "    if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=True,\n",
    "        batch_size=self.batch_size,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "        result = next(iter(self.train))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "    return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_STEPS = 1\n",
    "OUT_STEPS = 1\n",
    "\n",
    "multi_window = WindowGenerator(input_width=IN_STEPS,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS,\n",
    "                               label_columns=['est'],\n",
    "                               batch_size=1)\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b9590",
   "metadata": {},
   "source": [
    "# SPLITTING DATA INTO ZIPCODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_by_zc_tf = {}\n",
    "for zip_code in data_ohe_train.filter(like='zip').columns:\n",
    "    data_by_zc = data_ohe_train[data_ohe_train[zip_code]==1]\n",
    "    data_train_by_zc_tf[zip_code] = multi_window.make_dataset(data_by_zc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26369a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_by_zc_tf = {}\n",
    "for zip_code in data_ohe_test.filter(like='zip').columns:\n",
    "    data_by_zc = data_ohe_test[data_ohe_test[zip_code]==1]\n",
    "    data_test_by_zc_tf[zip_code] = multi_window.make_dataset(data_by_zc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9000c2",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units, out_steps):\n",
    "        super().__init__()\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(data_ohe_train.shape[1])\n",
    "        \n",
    "    def warmup(self, inputs):\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x)\n",
    "        return prediction, state\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        predictions = []\n",
    "        # Initialize the LSTM state.\n",
    "        prediction, state = self.warmup(inputs)\n",
    "\n",
    "        # Insert the first prediction.\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        # Run the rest of the prediction steps.\n",
    "        for n in range(1, self.out_steps):\n",
    "            # Use the last prediction as input.\n",
    "            x = prediction\n",
    "            # Execute one lstm step.\n",
    "            x, state = self.lstm_cell(x, states=state,\n",
    "                                      training=training)\n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x)\n",
    "            # Add the prediction to the output.\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        predictions = tf.stack(predictions)\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e559c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 100\n",
    "KERAS_VERBOSITY = 0\n",
    "\n",
    "def compile_and_fit(model, data, patience=2):\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    \n",
    "    for epoch in tqdm(np.arange(MAX_EPOCHS)):\n",
    "        \n",
    "        loss_curr_epoch = 0\n",
    "        val_loss_curr_epoch = 0\n",
    "        i = 0\n",
    "        \n",
    "        data_train_by_zip = list(data_train_by_zc_tf.values())\n",
    "        data_test_by_zip = list(data_test_by_zc_tf.values())\n",
    "        \n",
    "        for i in np.arange(len(data_train_by_zip)):\n",
    "            \n",
    "            history = model.fit(data_train_by_zip[i], epochs=1, validation_data=data_test_by_zip[i], verbose=KERAS_VERBOSITY)\n",
    "            loss_curr_epoch += history.history['loss'][0]\n",
    "            val_loss_curr_epoch += history.history['val_loss'][0]\n",
    "            i += 1\n",
    "                \n",
    "        losses += [invert_est_standardization(loss_curr_epoch/i)]\n",
    "        val_losses += [invert_est_standardization(val_loss_curr_epoch/i)]\n",
    "                \n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd452115",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_model = FeedBack(units=200, out_steps=OUT_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e629ac6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses, val_losses = compile_and_fit(feedback_model, data_train_by_zc_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, len(losses) + 1), losses, label='train')\n",
    "plt.plot(np.arange(1, len(val_losses) + 1), val_losses, label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b286efc",
   "metadata": {},
   "source": [
    "# MAKE LONG-TERM PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53109ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIS_IN_STEPS = 2018-2012\n",
    "VIS_OUT_STEPS = 0\n",
    "\n",
    "vis_window = WindowGenerator(input_width=VIS_IN_STEPS,\n",
    "                             label_width=VIS_OUT_STEPS,\n",
    "                             shift=VIS_OUT_STEPS,\n",
    "                             label_columns=['est'],\n",
    "                             batch_size=1)\n",
    "vis_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_91901 = data_ohe_train[data_ohe_train['zip_91901.0']==1]\n",
    "data_91901 = vis_window.make_dataset(data_91901)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aa397",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AUTOREGRESSIVE_OUT_STEPS = 10\n",
    "\n",
    "plot_col_index = vis_window.column_indices['est']\n",
    "\n",
    "inputs = next(iter(data_91901))[0]\n",
    "\n",
    "predictions = []\n",
    "prediction, state = feedback_model.warmup(inputs)\n",
    "\n",
    "predictions.append(prediction)\n",
    "\n",
    "for n in range(1, AUTOREGRESSIVE_OUT_STEPS):\n",
    "    x = prediction\n",
    "    x, state = feedback_model.lstm_cell(x, states=state, training=None)\n",
    "    prediction = feedback_model.dense(x)\n",
    "    predictions.append(prediction)\n",
    "predictions = tf.stack(predictions)\n",
    "predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "# f, ax = plt.subplots()\n",
    "plt.plot(vis_window.input_indices + 2012, (inputs[0, :, plot_col_index]*train_std['est'])+train_mean['est'], label='Inputs', marker='.', zorder=-10)\n",
    "plt.plot(np.arange(VIS_IN_STEPS, VIS_IN_STEPS + AUTOREGRESSIVE_OUT_STEPS) + 2012, (predictions[0, :, plot_col_index]*train_std['est'])+train_mean['est'], label='Labels', marker='.', zorder=-10, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd4a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
